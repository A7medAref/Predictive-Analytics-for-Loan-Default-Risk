{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preprocessing import get_cleaned_data\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.conf import SparkConf\n",
    "# spark=SparkSession.builder\\\n",
    "#     .master(\"local[*]\")\\\n",
    "#     .appName(\"BD-Project\")\\\n",
    "#     .getOrCreate()\n",
    "# sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282686\n"
     ]
    }
   ],
   "source": [
    "data = get_cleaned_data()\n",
    "\n",
    "# select only the columns with target = 0\n",
    "data = data[data[\"TARGET\"] == 0]\n",
    "\n",
    "# convert numbers to categories\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].map({x: col + \"_\" + str(x) for x in data[col].unique()})\n",
    "        \n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num=None):\n",
    "    dataset = []\n",
    "    if num is None:\n",
    "        num = len(data)\n",
    "    for i in range(0, num):\n",
    "        dataset.append(list(map(str, data.iloc[i].values.tolist())))\n",
    "    print(\"Dataset created\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahme\\AppData\\Local\\Temp\\ipykernel_18132\\761593838.py:4: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m oht_ary \u001b[38;5;241m=\u001b[39m te\u001b[38;5;241m.\u001b[39mfit(dataset)\u001b[38;5;241m.\u001b[39mtransform(dataset, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m sparse_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(oht_ary, columns\u001b[38;5;241m=\u001b[39mte\u001b[38;5;241m.\u001b[39mcolumns_)\n\u001b[1;32m----> 5\u001b[0m frequent_itemsets \u001b[38;5;241m=\u001b[39m apriori(\u001b[43mdf\u001b[49m, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, use_colnames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m frequent_itemsets\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/frequent_itemsets_0.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(None)\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True, low_memory=True)\n",
    "frequent_itemsets.to_csv(\"../data/frequent_itemsets_0.csv\")\n",
    "# frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "# frequent_itemsets = frequent_itemsets[ (frequent_itemsets['support'] >= 0.9) & (frequent_itemsets['length'] >= 4)]\n",
    "# frequent_itemsets.to_csv(\"../data/frequent_itemsets_1_support.csv\")\n",
    "# frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_C1(dataset):\n",
    "#     # Create a list of candidate item sets of size 1\n",
    "#     C1 = []\n",
    "#     for transaction in dataset:\n",
    "#         for item in transaction:\n",
    "#             if not [item] in C1:\n",
    "#                 C1.append([item])\n",
    "#     return list(map(frozenset, C1))\n",
    "\n",
    "# def scan_dataset(dataset, candidates, min_support):\n",
    "#     # Count the occurrence of each candidate item set in the dataset\n",
    "#     item_count = {}\n",
    "#     for transaction in dataset:\n",
    "#         for candidate in candidates:\n",
    "#             if candidate.issubset(transaction):\n",
    "#                 if candidate not in item_count:\n",
    "#                     item_count[candidate] = 1\n",
    "#                 else:\n",
    "#                     item_count[candidate] += 1\n",
    "\n",
    "#     num_items = float(len(dataset))\n",
    "#     frequent_items = []\n",
    "#     support_data = {}\n",
    "#     # Calculate support and filter out candidates below min_support\n",
    "#     for item, count in item_count.items():\n",
    "#         support = count / num_items\n",
    "#         if support >= min_support:\n",
    "#             frequent_items.insert(0, item)\n",
    "#             support_data[item] = support\n",
    "#     return frequent_items, support_data\n",
    "\n",
    "# def apriori_gen(freq_sets, k):\n",
    "#     # Generate candidate item sets of size k\n",
    "#     candidates = []\n",
    "#     len_freq_sets = len(freq_sets)\n",
    "#     for i in range(len_freq_sets):\n",
    "#         for j in range(i + 1, len_freq_sets):\n",
    "#             L1 = list(freq_sets[i])[:k - 2]\n",
    "#             L2 = list(freq_sets[j])[:k - 2]\n",
    "#             L1.sort()\n",
    "#             L2.sort()\n",
    "#             if L1 == L2:\n",
    "#                 candidates.append(freq_sets[i] | freq_sets[j])\n",
    "#     return candidates\n",
    "\n",
    "# def my_apriori(dataset, min_support=0.5, max_iterations=2):\n",
    "#     C1 = create_C1(dataset)\n",
    "#     D = list(map(set, dataset))\n",
    "#     k = 2\n",
    "#     L1, support_data = scan_dataset(D, C1, min_support)\n",
    "#     L = [L1]\n",
    "#     while len(L[k - 2]) > 0 and max_iterations > 0:\n",
    "#         candidates = apriori_gen(L[k - 2], k)\n",
    "#         print(f\"Scanning dataset For C{k - 2}\")\n",
    "#         Lk, supK = scan_dataset(D, candidates, min_support)\n",
    "#         support_data.update(supK)\n",
    "#         L.append(Lk)\n",
    "#         k += 1\n",
    "#         max_iterations -= 1\n",
    "#     return L, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created\n",
      "Scanning dataset For C0\n",
      "Scanning dataset For C1\n",
      "Scanning dataset For C2\n",
      "Frequent Item Sets: 4\n",
      "Support Data: 8539\n"
     ]
    }
   ],
   "source": [
    "# dataset = create_dataset(1000)\n",
    "# print(\"Dataset Created\")\n",
    "# # print(dataset)\n",
    "# max_iterations = 3\n",
    "# L, support_data = my_apriori(dataset, min_support=0.5, max_iterations=max_iterations)\n",
    "# print(f\"Frequent Item Sets: {len(L)}\")\n",
    "# # print(L)\n",
    "# print(f\"Support Data: {len(support_data)}\")\n",
    "# # print(support_data)\n",
    "# file = open(f'../data/support_data'+ str(max_iterations) +'.txt','w')\n",
    "# for item, val in support_data.items():\n",
    "# \tfile.write(str(item) + \" : \" + str(val) + \"\\n\")\n",
    "# file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
