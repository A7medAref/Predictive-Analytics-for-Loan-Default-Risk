{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, fbeta_score, accuracy_score\n",
    "from data_preprocessing import get_cleaned_data_final\n",
    "\n",
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model, model_Name, model_sample_method, dataset, X_test_scaled, y_test):\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "     # accuracy of model on test data\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n",
    "    \n",
    "    \n",
    "    # precision of model on test data\n",
    "    pre_test = precision_score(y_test, y_pred)\n",
    "    \n",
    "    # recall of model on test data\n",
    "    rec_test = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # f1 of model on test data\n",
    "    f1_test = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # f2 of model on test data\n",
    "    f2_test = fbeta_score(y_test, y_pred, beta=2, average='macro')\n",
    "    \n",
    "    \n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'f2 score: {f2_test}')\n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    d = {'Name': [model_Name], 'Sampling_Method': [model_sample_method], 'Dataset': [dataset],\n",
    "         'Accuracy': [acc_test], 'Precision': [pre_test], 'Recall': [rec_test],\n",
    "         'F1_Score': [f1_test], 'F2_Score': [f2_test]}\n",
    "    \n",
    "    return pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_under_sampling_data(x_train, y_train):\n",
    "    rus = RandomUnderSampler(random_state = 0)\n",
    "    X_rus, y_rus = rus.fit_resample(x_train, y_train)\n",
    "    scaler = RobustScaler().fit(X_rus)\n",
    "    X_train_scaled = scaler.transform(X_rus)\n",
    "\n",
    "    return X_train_scaled, y_rus, scaler\n",
    "\n",
    "def get_over_sampling_data(x_train, y_train):\n",
    "    smote_enn = SMOTEENN(smote=SMOTE(sampling_strategy='minority'))    \n",
    "    \n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(x_train, y_train)\n",
    "    scaler = RobustScaler().fit(X_resampled)\n",
    "    X_train_scaled = scaler.transform(X_resampled)\n",
    "\n",
    "    return X_train_scaled, y_resampled, scaler\n",
    "\n",
    "def evaluate_models(models_sampled_definitions, x_train, y_train, x_test, y_test):\n",
    "    # Get the under sampling data\n",
    "    X_train_scaled_under_sampling, y_train_under_sampling, scaler_under_sampling = get_under_sampling_data(x_train, y_train)\n",
    "    X_test_scaled_under_sampling = scaler_under_sampling.transform(x_test)\n",
    "\n",
    "    # Get the over sampling data\n",
    "    X_train_scaled_over_sampling, y_train_over_sampling, scaler_over_sampling = get_over_sampling_data(x_train, y_train)\n",
    "    X_test_scaled_over_sampling = scaler_over_sampling.transform(x_test)\n",
    "\n",
    "    # Evaluate the models\n",
    "    evaluation = []\n",
    "    under_sampling = 'Under Sampling'\n",
    "    over_sampling = 'Over Sampling'\n",
    "    train_dataset = 'Train'\n",
    "    test_dataset = 'Test'\n",
    "    for model_definition in models_sampled_definitions:\n",
    "        under_sampling_model = model_definition[0].fit(X_train_scaled_under_sampling, y_train_under_sampling)\n",
    "        model_name = model_definition[0].__class__.__name__\n",
    "\n",
    "\n",
    "        print(f'\\nUnder Sampling Model Name: {under_sampling_model.__class__.__name__}')\n",
    "        print(f'Train Data Evaluation')\n",
    "        evaluation_under_sampling_train = model_Evaluate(under_sampling_model, model_name, under_sampling, train_dataset, X_train_scaled_under_sampling, y_train_under_sampling)\n",
    "\n",
    "        print('\\nTest Data Evaluation')\n",
    "        evaluation_under_sampling_test = model_Evaluate(under_sampling_model, model_name, under_sampling, test_dataset, X_test_scaled_under_sampling, y_test)\n",
    "\n",
    "        over_sampling_model = model_definition[1].fit(X_train_scaled_over_sampling, y_train_over_sampling)\n",
    "        print(f'\\nOver Sampling Model Name: {over_sampling_model.__class__.__name__}')\n",
    "        print(f'Train Data Evaluation')\n",
    "        evaluation_over_sampling_train = model_Evaluate(over_sampling_model, model_name, over_sampling, train_dataset, X_train_scaled_over_sampling, y_train_over_sampling)\n",
    "        print('\\nTest Data Evaluation')\n",
    "        evaluation_over_sampling_test = model_Evaluate(over_sampling_model, model_name, over_sampling, test_dataset, X_test_scaled_over_sampling, y_test)\n",
    "\n",
    "        evaluation.append(evaluation_under_sampling_train)\n",
    "        evaluation.append(evaluation_under_sampling_test)\n",
    "        evaluation.append(evaluation_over_sampling_train)\n",
    "        evaluation.append(evaluation_over_sampling_test)\n",
    "\n",
    "    return evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data normally to get the field docs\n",
    "df = get_cleaned_data_final(convert_categorical=True)\n",
    "\n",
    "X = df.drop(['TARGET'],axis = 1)\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Under Sampling Model Name: XGBClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 84.59993899959332 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85     19672\n",
      "           1       0.85      0.84      0.85     19672\n",
      "\n",
      "    accuracy                           0.85     39344\n",
      "   macro avg       0.85      0.85      0.85     39344\n",
      "weighted avg       0.85      0.85      0.85     39344\n",
      "\n",
      "f2 score: 0.8459983763638652\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 64.3787881290152 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.64      0.77     55592\n",
      "           1       0.14      0.63      0.23      4959\n",
      "\n",
      "    accuracy                           0.64     60551\n",
      "   macro avg       0.54      0.64      0.50     60551\n",
      "weighted avg       0.88      0.64      0.72     60551\n",
      "\n",
      "f2 score: 0.5280243022694093\n",
      "\n",
      "Over Sampling Model Name: XGBClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 94.39217867283318 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93    138337\n",
      "           1       0.99      0.92      0.95    203293\n",
      "\n",
      "    accuracy                           0.94    341630\n",
      "   macro avg       0.94      0.95      0.94    341630\n",
      "weighted avg       0.95      0.94      0.94    341630\n",
      "\n",
      "f2 score: 0.9464678257749515\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 89.11991544318013 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     55592\n",
      "           1       0.22      0.13      0.16      4959\n",
      "\n",
      "    accuracy                           0.89     60551\n",
      "   macro avg       0.57      0.54      0.55     60551\n",
      "weighted avg       0.87      0.89      0.88     60551\n",
      "\n",
      "f2 score: 0.5448527789892029\n",
      "\n",
      "Under Sampling Model Name: RandomForestClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 88.47854819032128 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88     19672\n",
      "           1       0.87      0.90      0.89     19672\n",
      "\n",
      "    accuracy                           0.88     39344\n",
      "   macro avg       0.89      0.88      0.88     39344\n",
      "weighted avg       0.89      0.88      0.88     39344\n",
      "\n",
      "f2 score: 0.8847387603711276\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 65.97578900431041 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78     55592\n",
      "           1       0.14      0.64      0.24      4959\n",
      "\n",
      "    accuracy                           0.66     60551\n",
      "   macro avg       0.55      0.65      0.51     60551\n",
      "weighted avg       0.89      0.66      0.74     60551\n",
      "\n",
      "f2 score: 0.5420143324298031\n",
      "\n",
      "Over Sampling Model Name: RandomForestClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 91.1527090712174 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89    138337\n",
      "           1       0.95      0.90      0.92    203293\n",
      "\n",
      "    accuracy                           0.91    341630\n",
      "   macro avg       0.91      0.91      0.91    341630\n",
      "weighted avg       0.91      0.91      0.91    341630\n",
      "\n",
      "f2 score: 0.9120083078485879\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 84.21000478935112 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     55592\n",
      "           1       0.14      0.17      0.15      4959\n",
      "\n",
      "    accuracy                           0.84     60551\n",
      "   macro avg       0.53      0.54      0.53     60551\n",
      "weighted avg       0.86      0.84      0.85     60551\n",
      "\n",
      "f2 score: 0.5349546327616228\n",
      "\n",
      "Under Sampling Model Name: AdaBoostClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 66.01006506710044 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66     19672\n",
      "           1       0.66      0.66      0.66     19672\n",
      "\n",
      "    accuracy                           0.66     39344\n",
      "   macro avg       0.66      0.66      0.66     39344\n",
      "weighted avg       0.66      0.66      0.66     39344\n",
      "\n",
      "f2 score: 0.6600935667564227\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 65.30527984674076 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.65      0.78     55592\n",
      "           1       0.14      0.64      0.23      4959\n",
      "\n",
      "    accuracy                           0.65     60551\n",
      "   macro avg       0.55      0.65      0.50     60551\n",
      "weighted avg       0.89      0.65      0.73     60551\n",
      "\n",
      "f2 score: 0.5366705204824996\n",
      "\n",
      "Over Sampling Model Name: AdaBoostClassifier\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 90.87111787606474 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89    138337\n",
      "           1       0.94      0.90      0.92    203293\n",
      "\n",
      "    accuracy                           0.91    341630\n",
      "   macro avg       0.90      0.91      0.91    341630\n",
      "weighted avg       0.91      0.91      0.91    341630\n",
      "\n",
      "f2 score: 0.9083997772263925\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 84.42635134019257 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91     55592\n",
      "           1       0.15      0.20      0.17      4959\n",
      "\n",
      "    accuracy                           0.84     60551\n",
      "   macro avg       0.54      0.55      0.54     60551\n",
      "weighted avg       0.86      0.84      0.85     60551\n",
      "\n",
      "f2 score: 0.5462353054772522\n",
      "\n",
      "Under Sampling Model Name: GaussianNB\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 62.29412362749085 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.61      0.62     19672\n",
      "           1       0.62      0.63      0.63     19672\n",
      "\n",
      "    accuracy                           0.62     39344\n",
      "   macro avg       0.62      0.62      0.62     39344\n",
      "weighted avg       0.62      0.62      0.62     39344\n",
      "\n",
      "f2 score: 0.6229058605976007\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 61.273967399382336 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.61      0.74     55592\n",
      "           1       0.13      0.63      0.21      4959\n",
      "\n",
      "    accuracy                           0.61     60551\n",
      "   macro avg       0.54      0.62      0.48     60551\n",
      "weighted avg       0.88      0.61      0.70     60551\n",
      "\n",
      "f2 score: 0.5038576240196233\n",
      "\n",
      "Over Sampling Model Name: GaussianNB\n",
      "Train Data Evaluation\n",
      "Accuracy of model on testing data : 75.19714310804086 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67    138337\n",
      "           1       0.77      0.83      0.80    203293\n",
      "\n",
      "    accuracy                           0.75    341630\n",
      "   macro avg       0.74      0.73      0.74    341630\n",
      "weighted avg       0.75      0.75      0.75    341630\n",
      "\n",
      "f2 score: 0.7340038669929474\n",
      "\n",
      "Test Data Evaluation\n",
      "Accuracy of model on testing data : 59.76945054582088 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.61      0.74     55592\n",
      "           1       0.09      0.46      0.16      4959\n",
      "\n",
      "    accuracy                           0.60     60551\n",
      "   macro avg       0.51      0.53      0.45     60551\n",
      "weighted avg       0.86      0.60      0.69     60551\n",
      "\n",
      "f2 score: 0.4565999208474354\n"
     ]
    }
   ],
   "source": [
    "xgb_model_definition_under_sampling = xgb.XGBClassifier(n_estimators=50, max_depth=8)\n",
    "xgb_model_definition_over_sampling = xgb.XGBClassifier(n_estimators=50, max_depth=8)\n",
    "\n",
    "rf_model_definition_under_sampling = RandomForestClassifier(n_estimators = 300, criterion = 'entropy', max_depth=15, class_weight='balanced')\n",
    "rf_model_definition_over_sampling = RandomForestClassifier(n_estimators = 300, criterion = 'entropy', max_depth=15, class_weight='balanced')\n",
    "\n",
    "ada_model_definition_under_sampling = AdaBoostClassifier(\n",
    "    base_estimator=None,\n",
    "    n_estimators=300,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=42\n",
    ")\n",
    "ada_model_definition_over_sampling = AdaBoostClassifier(\n",
    "    base_estimator=None,\n",
    "    n_estimators=250,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "bayes_model_definition_under_sampling = GaussianNB()\n",
    "bayes_model_definition_over_sampling = GaussianNB()\n",
    "\n",
    "\n",
    "# Evaluate this model only\n",
    "evaluation = evaluate_models([\n",
    "        (xgb_model_definition_under_sampling, xgb_model_definition_over_sampling),\n",
    "        (rf_model_definition_under_sampling, rf_model_definition_over_sampling),\n",
    "        (ada_model_definition_under_sampling, ada_model_definition_over_sampling),\n",
    "        (bayes_model_definition_under_sampling, bayes_model_definition_over_sampling)\n",
    "    ], X_train_raw, y_train_raw, X_test_raw, y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Precision model Name: XGBClassifier\n",
      "Best Recall model Name: RandomForestClassifier\n",
      "Best F1 model Name: RandomForestClassifier\n",
      "Best F2 model Name: AdaBoostClassifier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sampling_Method</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>F2_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.845999</td>\n",
       "      <td>0.847359</td>\n",
       "      <td>0.844042</td>\n",
       "      <td>0.845697</td>\n",
       "      <td>0.845998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.643788</td>\n",
       "      <td>0.136956</td>\n",
       "      <td>0.631781</td>\n",
       "      <td>0.225112</td>\n",
       "      <td>0.528024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.943922</td>\n",
       "      <td>0.985432</td>\n",
       "      <td>0.919353</td>\n",
       "      <td>0.951246</td>\n",
       "      <td>0.946468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.891199</td>\n",
       "      <td>0.216893</td>\n",
       "      <td>0.125832</td>\n",
       "      <td>0.159265</td>\n",
       "      <td>0.544853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.884785</td>\n",
       "      <td>0.874709</td>\n",
       "      <td>0.898231</td>\n",
       "      <td>0.886314</td>\n",
       "      <td>0.884739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.659758</td>\n",
       "      <td>0.144299</td>\n",
       "      <td>0.639847</td>\n",
       "      <td>0.235491</td>\n",
       "      <td>0.542014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.911527</td>\n",
       "      <td>0.948735</td>\n",
       "      <td>0.899952</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.912008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.135572</td>\n",
       "      <td>0.172615</td>\n",
       "      <td>0.151867</td>\n",
       "      <td>0.534955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.660101</td>\n",
       "      <td>0.661687</td>\n",
       "      <td>0.655195</td>\n",
       "      <td>0.658425</td>\n",
       "      <td>0.660094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.653053</td>\n",
       "      <td>0.141555</td>\n",
       "      <td>0.639040</td>\n",
       "      <td>0.231771</td>\n",
       "      <td>0.536671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.908711</td>\n",
       "      <td>0.941583</td>\n",
       "      <td>0.902589</td>\n",
       "      <td>0.921674</td>\n",
       "      <td>0.908400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.844264</td>\n",
       "      <td>0.151846</td>\n",
       "      <td>0.196612</td>\n",
       "      <td>0.171353</td>\n",
       "      <td>0.546235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.620329</td>\n",
       "      <td>0.633794</td>\n",
       "      <td>0.626990</td>\n",
       "      <td>0.622906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Under Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.612740</td>\n",
       "      <td>0.126011</td>\n",
       "      <td>0.628151</td>\n",
       "      <td>0.209913</td>\n",
       "      <td>0.503858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.751971</td>\n",
       "      <td>0.769182</td>\n",
       "      <td>0.833231</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.734004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Over Sampling</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.597695</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.455535</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>0.456600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name Sampling_Method Dataset  Accuracy  Precision  \\\n",
       "0           XGBClassifier  Under Sampling   Train  0.845999   0.847359   \n",
       "0           XGBClassifier  Under Sampling    Test  0.643788   0.136956   \n",
       "0           XGBClassifier   Over Sampling   Train  0.943922   0.985432   \n",
       "0           XGBClassifier   Over Sampling    Test  0.891199   0.216893   \n",
       "0  RandomForestClassifier  Under Sampling   Train  0.884785   0.874709   \n",
       "0  RandomForestClassifier  Under Sampling    Test  0.659758   0.144299   \n",
       "0  RandomForestClassifier   Over Sampling   Train  0.911527   0.948735   \n",
       "0  RandomForestClassifier   Over Sampling    Test  0.842100   0.135572   \n",
       "0      AdaBoostClassifier  Under Sampling   Train  0.660101   0.661687   \n",
       "0      AdaBoostClassifier  Under Sampling    Test  0.653053   0.141555   \n",
       "0      AdaBoostClassifier   Over Sampling   Train  0.908711   0.941583   \n",
       "0      AdaBoostClassifier   Over Sampling    Test  0.844264   0.151846   \n",
       "0              GaussianNB  Under Sampling   Train  0.622941   0.620329   \n",
       "0              GaussianNB  Under Sampling    Test  0.612740   0.126011   \n",
       "0              GaussianNB   Over Sampling   Train  0.751971   0.769182   \n",
       "0              GaussianNB   Over Sampling    Test  0.597695   0.094444   \n",
       "\n",
       "     Recall  F1_Score  F2_Score  \n",
       "0  0.844042  0.845697  0.845998  \n",
       "0  0.631781  0.225112  0.528024  \n",
       "0  0.919353  0.951246  0.946468  \n",
       "0  0.125832  0.159265  0.544853  \n",
       "0  0.898231  0.886314  0.884739  \n",
       "0  0.639847  0.235491  0.542014  \n",
       "0  0.899952  0.923700  0.912008  \n",
       "0  0.172615  0.151867  0.534955  \n",
       "0  0.655195  0.658425  0.660094  \n",
       "0  0.639040  0.231771  0.536671  \n",
       "0  0.902589  0.921674  0.908400  \n",
       "0  0.196612  0.171353  0.546235  \n",
       "0  0.633794  0.626990  0.622906  \n",
       "0  0.628151  0.209913  0.503858  \n",
       "0  0.833231  0.799926  0.734004  \n",
       "0  0.455535  0.156451  0.456600  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all the evaluation metrics as one dataframe so the first column is the model name\n",
    "# So row after row only (Using loop)\n",
    "evaluation_df = pd.concat(evaluation)\n",
    "\n",
    "# Find the best precision, Recall, F1, F2 for dataset = Test\n",
    "best_precision = evaluation_df[evaluation_df['Dataset'] == 'Test'].sort_values('Precision', ascending=False)\n",
    "best_recall = evaluation_df[evaluation_df['Dataset'] == 'Test'].sort_values('Recall', ascending=False)\n",
    "best_f1 = evaluation_df[evaluation_df['Dataset'] == 'Test'].sort_values('F1_Score', ascending=False)\n",
    "best_f2 = evaluation_df[evaluation_df['Dataset'] == 'Test'].sort_values('F2_Score', ascending=False)\n",
    "\n",
    "\n",
    "print(f'Best Precision model Name: {best_precision.iloc[0][\"Name\"]}')\n",
    "print(f'Best Recall model Name: {best_recall.iloc[0][\"Name\"]}')\n",
    "print(f'Best F1 model Name: {best_f1.iloc[0][\"Name\"]}')\n",
    "print(f'Best F2 model Name: {best_f2.iloc[0][\"Name\"]}')\n",
    "\n",
    "evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
